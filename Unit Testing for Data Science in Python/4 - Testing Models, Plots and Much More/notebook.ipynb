{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e15564",
   "metadata": {},
   "source": [
    "## Use a fixture for a clean data file\n",
    "\n",
    "In the video, you saw how the preprocess() function creates a clean data file.\n",
    "\n",
    "The get_data_as_numpy_array() function takes the path to this clean data file as the first argument and the number of columns of data as the second argument. It returns a NumPy array holding the data.\n",
    "\n",
    "In a previous exercise, you wrote the test test_on_clean_file() without using a fixture. That's bad practice! This time, you'll use the fixture clean_data_file(), which\n",
    "\n",
    "- creates a clean data file in the setup,\n",
    "- yields the path to the clean data file,\n",
    "- removes the clean data file in the teardown.\n",
    "\n",
    "The contents of the clean data file that you will use for testing is printed in the IPython console.\n",
    "\n",
    "pytest, os, numpy as np and get_data_as_numpy_array() have been imported for you.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\n",
    "- Add the correct decorator that would turn clean_data_file() into a fixture.\n",
    "\n",
    "\n",
    "- Pass an argument to the test test_on_clean_file() so that it uses the fixture.\n",
    "\n",
    "\n",
    "- Pass the clean data file path yielded by the fixture as the first argument to the function get_data_as_numpy_array()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193499d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a decorator to make this function a fixture\n",
    "@pytest.fixture\n",
    "def clean_data_file():\n",
    "    file_path = \"clean_data_file.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
    "    yield file_path\n",
    "    os.remove(file_path)\n",
    "    \n",
    "# Pass the correct argument so that the test can use the fixture\n",
    "def test_on_clean_file(clean_data_file):\n",
    "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
    "    # Pass the clean data file path yielded by the fixture as the first argument\n",
    "    actual = get_data_as_numpy_array(clean_data_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1d40c",
   "metadata": {},
   "source": [
    "## Write a fixture for an empty data file\n",
    "\n",
    "When a function takes a data file as an argument, you need to write a fixture that takes care of creating and deleting that data file. This exercise will test your ability to write such a fixture.\n",
    "\n",
    "get_data_as_numpy_array() should return an empty numpy array if it gets an empty data file as an argument. To test this behavior, you need to write a fixture empty_file() that does the following.\n",
    "\n",
    "- Creates an empty data file empty.txt relative to the current working directory in setup.\n",
    "- Yields the path to the empty data file.\n",
    "- Deletes the empty data file in teardown.\n",
    "\n",
    "The fixture will be used by the test test_on_empty_file(), which is available for you to see in the script.\n",
    "\n",
    "os, pytest, numpy as np and get_data_as_numpy_array have been imported for you.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\n",
    "- In the setup, assign the variable file_path to the correct string.\n",
    "\n",
    "\n",
    "- After the setup, yield the variable file_path so that the test can use it.\n",
    "\n",
    "\n",
    "- In the teardown, remove the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7869949",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def empty_file():\n",
    "    # Assign the file path \"empty.txt\" to the variable\n",
    "    file_path = \"empty.txt\"\n",
    "    open(file_path, \"w\").close()\n",
    "    # Yield the variable file_path\n",
    "    yield file_path\n",
    "    # Remove the file in the teardown\n",
    "    os.remove(file_path)\n",
    "    \n",
    "def test_on_empty_file(self, empty_file):\n",
    "    expected = np.empty((0, 2))\n",
    "    actual = get_data_as_numpy_array(empty_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb44c6f",
   "metadata": {},
   "source": [
    "## Fixture chaining using tmpdir\n",
    "\n",
    "The built-in tmpdir fixture is very useful when dealing with files in setup and teardown. tmpdir combines seamlessly with user defined fixture via fixture chaining.\n",
    "\n",
    "In this exercise, you will use the power of tmpdir to redefine and improve the empty_file() fixture that you wrote in the last exercise and get some experience with fixture chaining.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\n",
    "- Add the correct argument to the fixture empty_file() so that it chains with the built-in fixture tmpdir.\n",
    "\n",
    "\n",
    "- Use the appropriate method to create an empty file \"empty.txt\" inside the temporary directory created by tmpdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "def empty_file(tmpdir):\n",
    "    # Use the appropriate method to create an empty file in the temporary directory\n",
    "    file_path = tmpdir.join(\"empty.txt\")\n",
    "    open(file_path, \"w\").close()\n",
    "    yield file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243856ae",
   "metadata": {},
   "source": [
    "## Program a bug-free dependency\n",
    "\n",
    "In the video, row_to_list() was mocked. But preprocess() has another dependency convert_to_int(). Generally, its best to mock all dependencies of the function under test. It's your job to mock convert_to_int() in this and the following exercises.\n",
    "\n",
    "The raw data file used in the test is printed in the IPython console. The second row \"1,767565,112\\n\" is dirty, so row_to_list() will filter it out. The rest will be converted to lists and convert_to_int() will process the areas and prices.\n",
    "\n",
    "The mocked convert_to_int() should process these areas and prices correctly. Here is the dictionary holding the correct return values.\n",
    "\n",
    "```\n",
    "{\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\n",
    "- Define a function convert_to_int_bug_free() which takes one argument called comma_separated_integer_string.\n",
    "\n",
    "\n",
    "- Assign return_values to the dictionary holding the correct return values in the context of the raw data file used in the test.\n",
    "\n",
    "\n",
    "- Return the correct return value by looking up the dictionary return_values for the key comma_separated_integer_string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function convert_to_int_bug_free\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dictionary holding the correct return values \n",
    "    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
    "    # Return the correct result using the dictionary return_values\n",
    "    return return_values[comma_separated_integer_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd2046",
   "metadata": {},
   "source": [
    "## Mock a dependency\n",
    "\n",
    "Mocking helps us replace a dependency with a MagicMock() object. Usually, the MagicMock() is programmed to be a bug-free version of the dependency. To verify whether the function under test works properly with the dependency, you simply check whether the MagicMock() is called with the correct arguments and in the right order.\n",
    "\n",
    "In the last exercise, you programmed a bug-free version of the dependency data.preprocessing_helpers.convert_to_int in the context of the test test_on_raw_data(), which applies preprocess() on a raw data file. The data file is printed out in the IPython console.\n",
    "\n",
    "pytest, unittest.mock.call, preprocess raw_and_clean_data_file and convert_to_int_bug_free has been imported for you.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\n",
    "- In the test test_on_raw_data(), add the correct argument that enables the use of the mocking fixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a349a",
   "metadata": {},
   "source": [
    "- Replace the dependency \"data.preprocessing_helpers.convert_to_int\" with the bug-free version convert_to_int_bug_free() by using the correct method and side effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                    side_effect = convert_to_int_bug_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08528092",
   "metadata": {},
   "source": [
    "- Use the correct attribute which returns the list of calls to the mock, and check if the mock was called with this sequence of arguments: \"1,801\", \"201,411\", \"2,002\", \"333,209\", \"1990\", \"782,911\", \"1,285\", \"389129\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b28e9",
   "metadata": {},
   "source": [
    "## Testing on circular data\n",
    "\n",
    "Another special case where it is easy to guess the value of r^2 is when the model does not fit the testing dataset at all. In this case, r^2 takes its lowest possible value 0.0.\n",
    "\n",
    "The plot shows such a testing dataset and model. The testing dataset consists of data arranged in a circle of radius 1.0. The x and y co-ordinates of the data is shown on the plot. The model corresponds to a straight line y=0.\n",
    "\n",
    "As one can easily see, the straight line does not fit the data at all. In this particular case, the value of r^2 is known to be 0.0.\n",
    "\n",
    "Your job is to write a test test_on_circular_data() for the function model_test() that performs this sanity check. pytest, numpy as np, model_test, sin, cos and pi have been imported for you.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\n",
    "- Assign test_argument to a NumPy array holding the circular testing data shown in the plot, starting with (1.0, 0.0) and moving anticlockwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Complete the NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], \n",
    "                              [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6fe060",
   "metadata": {},
   "source": [
    "- Fill in with the slope and intercept of the straight line shown in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d05007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Assign to a NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                             )\n",
    "    # Fill in with the slope and intercept of the straight line\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3282207",
   "metadata": {},
   "source": [
    "- Remembering that model_test() returns a float, complete the assert statement to check if model_test() returns the expected value of in this special case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aed81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Assign to a NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                             )\n",
    "    # Fill in with the slope and intercept of the straight line\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
